{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\canni\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils #One-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_accuracy_and_heatmap(model, X, y):\n",
    "#     cm = confusion_matrix(y,model.predict(X))\n",
    "#     sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "    ac = accuracy_score(y,model.predict_classes(X))\n",
    "    print('Accuracy is: ', ac)\n",
    "    print (pd.crosstab(y,model.predict_classes(X),rownames=['label'],colnames=['predict']))\n",
    "    ##metrics.confusion_matrix(y_test, clf_lr.predict(X_test_selected)) \n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import data\n",
    "os.chdir('D:/data2')\n",
    "path = \"D:/data2\" #資料夾目錄\n",
    "files= os.listdir(path) #得到資料夾下的所有檔名稱\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In  Assay 20.csv\n",
      "Nan in row  43  :  114\n",
      "    Unnamed: 0        V1        V2        V3        V4        V5        V6  \\\n",
      "0            1  1.704955  0.755850  1.076499  1.640432  2.844560  3.328134   \n",
      "1            2  1.484763  0.738368  1.040285  1.505723  2.686339  3.286160   \n",
      "2            3  1.662222  0.726194  0.993292  1.616492  2.710324  3.186518   \n",
      "3            4  1.543112  0.690364  0.963098  1.428123  2.351463  2.788580   \n",
      "4            5  1.752083  0.739432  1.014502  1.451558  2.496606  3.039571   \n",
      "5            6  1.578395  0.693596  0.931738  1.422942  2.207460  2.895355   \n",
      "6            7  1.593588  0.740670  0.976849  1.565033  2.427810  2.837873   \n",
      "7            8  1.474497  0.682559  0.916265  1.409606  2.174837  2.769269   \n",
      "8            9  1.962298  0.973986  1.400845  1.858015  2.423164  2.914568   \n",
      "9           10  2.150923  0.978806  1.483732  2.004222  2.592350  3.149516   \n",
      "10          11  1.897247  0.857523  1.272912  1.745464  2.440882  2.839146   \n",
      "11          12  1.820878  0.839062  1.212849  1.682188  2.233386  2.719909   \n",
      "12          13  1.777511  0.757378  1.127926  1.502980  2.287542  2.638610   \n",
      "13          14  1.744880  0.804096  1.177727  1.719717  2.486903  3.034212   \n",
      "14          15  1.732556  0.753380  1.076768  1.630148  2.414918  2.972960   \n",
      "15          16  1.723923  0.785383  1.146851  1.611639  2.273950  2.848135   \n",
      "16          17  1.731267  0.734128  1.018355  1.598888  2.435360  2.972745   \n",
      "17          18  1.650688  0.773407  1.159812  1.591684  2.319324  2.788663   \n",
      "18          19  1.552430  0.683524  1.027495  1.371887  2.148348  2.633022   \n",
      "19          20  1.695289  0.828104  1.147099  1.617767  2.397480  2.930824   \n",
      "20          21  1.694319  0.733435  1.096149  1.510560  2.357987  2.907434   \n",
      "21          22  1.694471  0.795353  1.151843  1.631891  2.361218  3.033333   \n",
      "22          23  1.588248  0.763432  1.023256  1.536633  2.321572  2.874535   \n",
      "23          24  1.566436  0.721098  1.048677  1.494694  2.205391  2.613627   \n",
      "24          25  6.114026  4.191599  7.468174  5.257987  2.636759  3.355355   \n",
      "25          26  6.524794  4.391730  7.679993  5.436685  2.704444  3.441318   \n",
      "26          27  5.138659  3.088164  5.072414  4.352817  2.558695  3.005520   \n",
      "27          28  5.664288  3.474387  5.950970  4.844859  2.933407  3.545371   \n",
      "28          29  3.956664  2.165579  3.537512  3.453246  2.518562  3.126670   \n",
      "29          30  3.742524  2.131728  3.553289  3.393958  2.462966  2.847723   \n",
      "30          31  3.040043  1.540652  2.363581  2.749391  2.446711  2.944275   \n",
      "31          32  2.788749  1.560300  2.470451  2.700357  2.283174  2.775632   \n",
      "32          33  1.704893  0.794258  1.151104  1.615990  2.340022  3.091990   \n",
      "33          34  1.673023  0.732820  1.059797  1.443902  2.270841  2.836778   \n",
      "34          35  1.697392  0.824455  1.194186  1.693031  2.617272  3.259256   \n",
      "35          36  1.604406  0.708738  1.090515  1.468335  2.293055  2.870948   \n",
      "36          37  1.560578  0.704332  1.035162  1.466137  2.290542  2.760505   \n",
      "37          38  1.715057  0.784189  1.087864  1.558060  2.314787  3.027145   \n",
      "38          39  1.636559  0.734212  1.029466  1.481676  2.223144  2.743837   \n",
      "39          40  1.544572  0.772059  1.066859  1.500606  2.295861  2.838008   \n",
      "40          41  1.486039  0.695779  0.971220  1.486896  2.207313  2.652789   \n",
      "41          42  1.758394  0.838367  1.239244  1.659862  2.429813  3.226000   \n",
      "42          43       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "43          44  1.591373  0.719139  1.087909  1.488415  2.147434  2.719213   \n",
      "44          45  1.559255  0.743748  1.066819  1.557279  2.340555  2.808134   \n",
      "45          46  1.638440  0.785659  1.066656  1.582505  2.229646  2.744562   \n",
      "46          47  1.628196  0.705812  1.060592  1.457218  2.214923  2.873416   \n",
      "\n",
      "          V7        V8        V9  ...      V108      V109      V110      V111  \\\n",
      "0   0.825100  1.372568  1.523674  ...  0.382096  1.471935  0.484735  1.250994   \n",
      "1   0.799688  1.303181  1.476587  ...  0.381094  1.406146  0.441563  1.252573   \n",
      "2   0.734948  1.196008  1.534522  ...  0.365191  1.372927  0.479489  1.301674   \n",
      "3   0.658958  1.131263  1.391506  ...  0.383831  1.420366  0.469444  1.300914   \n",
      "4   0.718297  1.139772  1.584696  ...  0.397870  1.414963  0.496026  1.300756   \n",
      "5   0.688670  1.090077  1.425405  ...  0.406223  1.474116  0.480284  1.437105   \n",
      "6   0.658585  1.087198  1.453005  ...  0.379830  1.352064  0.440539  1.298002   \n",
      "7   0.608954  1.082743  1.431425  ...  0.351152  1.294195  0.436907  1.267430   \n",
      "8   0.708415  1.110768  1.531217  ...  0.397081  1.340746  0.444008  1.168967   \n",
      "9   0.683749  1.169716  1.615283  ...  0.400327  1.417855  0.471161  1.297973   \n",
      "10  0.688614  1.096966  1.488583  ...  0.386588  1.539524  0.497622  1.434668   \n",
      "11  0.711851  1.091343  1.512764  ...  0.403472  1.400342  0.501437  1.382619   \n",
      "12  0.630833  1.019480  1.486553  ...  0.380990  1.334716  0.472322  1.296488   \n",
      "13  0.731465  1.057390  1.583188  ...  0.400734  1.388282  0.459030  1.344884   \n",
      "14  0.653768  1.064880  1.569075  ...  0.350504  1.307205  0.432637  1.268732   \n",
      "15  0.638760  1.040796  1.491605  ...  0.355876  1.260635  0.400343  1.192235   \n",
      "16  0.723878  1.067541  2.011998  ...  0.341001  1.272271  0.422736  1.173469   \n",
      "17  0.680302  1.052946  1.885432  ...  0.378694  1.316217  0.426607  1.219612   \n",
      "18  0.627692  0.981342  1.539384  ...  0.382532  1.346244  0.461862  1.224194   \n",
      "19  0.672739  1.100794  1.798491  ...  0.370165  1.320071  0.471898  1.295484   \n",
      "20  0.612276  1.053984  1.579260  ...  0.321645  1.263329  0.420334  1.188645   \n",
      "21  0.658093  1.150962  1.651923  ...  0.365737  1.359904  0.461901  1.336312   \n",
      "22  0.680761  1.056208  1.535759  ...  0.322324  1.165581  0.422280  1.124612   \n",
      "23  0.614900  1.052781  1.516485  ...  0.337980  1.382676  0.455819  1.247733   \n",
      "24  0.761648  1.192323  2.009254  ...  0.463795  1.574423  0.535378  1.511217   \n",
      "25  0.776939  1.251544  2.109300  ...  0.480982  1.625966  0.543491  1.636763   \n",
      "26  0.730800  1.133626  1.719435  ...  0.435877  1.443576  0.550678  1.552976   \n",
      "27  0.853641  1.301683  2.101720  ...  0.410431  1.444998  0.512353  1.478640   \n",
      "28  0.724293  1.102439  1.722507  ...  0.399596  1.412291  0.514209  1.380338   \n",
      "29  0.706487  1.096151  1.661402  ...  0.384097  1.336611  0.449705  1.302789   \n",
      "30  0.696102  1.109318  1.611906  ...  0.348345  1.200658  0.433447  1.196142   \n",
      "31  0.689208  1.105735  1.493405  ...  0.317234  1.279786  0.409450  1.173518   \n",
      "32  0.759068  1.112980  1.580954  ...  0.464813  1.494746  0.545322  1.239839   \n",
      "33  0.637861  1.055068  1.437343  ...  0.445365  1.553467  0.512302  1.188956   \n",
      "34  0.748012  1.302608  1.657289  ...  0.410590  1.474288  0.513691  1.208164   \n",
      "35  0.650485  1.069156  1.531143  ...  0.448262  1.534263  0.548689  1.337609   \n",
      "36  0.600578  0.998700  1.408087  ...  0.378121  1.344477  0.467379  1.264600   \n",
      "37  0.670053  1.075006  1.525756  ...  0.367735  1.337186  0.463151  1.255232   \n",
      "38  0.639890  1.057156  1.435256  ...  0.349255  1.235290  0.447475  1.186852   \n",
      "39  0.656155  1.110218  1.386825  ...  0.349420  1.323029  0.451968  1.147058   \n",
      "40  0.618796  1.036135  1.438620  ...  0.358373  1.260721  0.395233  1.129139   \n",
      "41  0.727349  1.185423  1.549350  ...  0.365962  1.199547  0.425298  1.198450   \n",
      "42       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "43  0.646726  0.996275  1.449378  ...  0.344193  1.305228  0.436690  1.197122   \n",
      "44  0.639453  1.045838  1.500190  ...  0.359495  1.256511  0.454194  1.166515   \n",
      "45  0.680392  1.053449  1.540009  ...  0.339680  1.234018  0.438960  1.165975   \n",
      "46  0.676415  1.127409  1.518860  ...  0.343754  1.300953  0.435056  1.220395   \n",
      "\n",
      "        V112      V113      V114  V115  V116  V117  \n",
      "0   0.668190  0.553983  0.640642     2     2     3  \n",
      "1   0.668351  0.572245  0.593341     2     2     3  \n",
      "2   0.672459  0.489249  0.613029     2     2     3  \n",
      "3   0.683651  0.521726  0.637465     2     2     3  \n",
      "4   0.670808  0.623196  0.660699     2     2     3  \n",
      "5   0.712228  0.640002  0.720700     2     2     3  \n",
      "6   0.661672  0.600558  0.630380     2     2     3  \n",
      "7   0.634242  0.574913  0.624002     2     2     3  \n",
      "8   0.639647  0.539737  0.649342     2     2     3  \n",
      "9   0.700011  0.581841  0.657884     2     2     3  \n",
      "10  0.731396  0.590937  0.697379     2     2     3  \n",
      "11  0.725099  0.576622  0.666946     2     2     3  \n",
      "12  0.658847  0.601917  0.613861     2     2     3  \n",
      "13  0.728199  0.608976  0.644146     2     2     3  \n",
      "14  0.657457  0.551333  0.588977     2     2     3  \n",
      "15  0.663466  0.563810  0.595522     2     2     3  \n",
      "16  0.631047  0.461712  0.575497     2     2     3  \n",
      "17  0.662698  0.641293  0.634276     2     2     3  \n",
      "18  0.661840  0.603061  0.621744     2     2     3  \n",
      "19  0.690530  0.578306  0.635504     2     2     3  \n",
      "20  0.670993  0.581824  0.579769     2     2     3  \n",
      "21  0.687430  0.616840  0.642825     2     2     3  \n",
      "22  0.615153  0.558492  0.549560     2     2     3  \n",
      "23  0.645557  0.611386  0.628880     2     2     3  \n",
      "24  0.744841  0.726541  0.863378     2     2     3  \n",
      "25  0.813798  0.706395  0.852516     2     2     3  \n",
      "26  0.835823  0.688807  0.803762     2     2     3  \n",
      "27  0.743788  0.648416  0.716269     2     2     3  \n",
      "28  0.754256  0.604696  0.701205     2     2     3  \n",
      "29  0.684357  0.650393  0.657418     2     2     3  \n",
      "30  0.635880  0.542196  0.595909     2     2     3  \n",
      "31  0.624600  0.578557  0.583099     2     2     3  \n",
      "32  0.673469  0.617048  0.663637     2     2     3  \n",
      "33  0.624665  0.565391  0.605354     2     2     3  \n",
      "34  0.678058  0.609675  0.648315     2     2     3  \n",
      "35  0.718423  0.620487  0.661238     2     2     3  \n",
      "36  0.695232  0.595359  0.621033     2     2     3  \n",
      "37  0.663424  0.577521  0.592849     2     2     3  \n",
      "38  0.657574  0.595584  0.601705     2     2     3  \n",
      "39  0.617881  0.573266  0.605264     2     2     3  \n",
      "40  0.562665  0.524828  0.586272     2     2     3  \n",
      "41  0.625292  0.546508  0.593394     2     2     3  \n",
      "42       NaN       NaN       NaN     2     2     3  \n",
      "43  0.655556  0.578666  0.603602     2     2     3  \n",
      "44  0.686486  0.527413  0.614461     2     2     3  \n",
      "45  0.624934  0.572421  0.579619     2     2     3  \n",
      "46  0.675453  0.583978  0.622288     2     2     3  \n",
      "\n",
      "[47 rows x 118 columns]\n",
      "In  Assay 22.csv\n",
      "Nan in row  28  :  114\n",
      "    Unnamed: 0        V1        V2        V3        V4        V5        V6  \\\n",
      "0            1  1.607165  0.686559  0.962943  1.533580  2.702667  3.305695   \n",
      "1            2  1.620747  0.720969  1.090422  1.476701  2.698780  3.352441   \n",
      "2            3  1.547143  0.692730  1.105961  1.483283  2.488236  3.226340   \n",
      "3            4  1.494309  0.723089  1.079675  1.489106  2.376260  2.960163   \n",
      "4            5  1.795314  0.800521  1.180486  1.608754  2.615310  3.381026   \n",
      "5            6  1.701727  0.697323  1.069257  1.521848  2.399396  3.099396   \n",
      "6            7  1.595307  0.711486  1.152650  1.557167  2.420951  3.184902   \n",
      "7            8  1.530289  0.721596  1.049479  1.549691  2.321528  2.990341   \n",
      "8            9  2.173913  0.989242  1.492694  1.949529  2.274585  3.071537   \n",
      "9           10  2.030874  0.996667  1.629068  1.940093  2.343040  2.945970   \n",
      "10          11  1.981999  0.916422  1.446914  1.848641  2.485121  3.140154   \n",
      "11          12  1.945946  0.818520  1.309598  1.706861  2.408437  2.965350   \n",
      "12          13  1.870217  0.837726  1.286643  1.748285  2.474007  3.116065   \n",
      "13          14  1.825324  0.791847  1.207136  1.622426  2.366980  2.908213   \n",
      "14          15  1.847512  0.845015  1.230337  1.694667  2.464419  3.175495   \n",
      "15          16  1.690840  0.741122  1.105709  1.581231  2.104049  2.772735   \n",
      "16          17  1.548340  0.713586  0.906766  1.548159  2.415200  3.014330   \n",
      "17          18  1.714127  0.774842  1.116426  1.563311  2.308954  3.186596   \n",
      "18          19  1.634085  0.774369  1.150421  1.597701  2.376957  3.034573   \n",
      "19          20  1.539940  0.680804  1.067959  1.449112  2.298678  2.949801   \n",
      "20          21  1.691665  0.791810  1.160251  1.639368  2.524605  3.182041   \n",
      "21          22  1.807764  0.765856  1.207951  1.640131  2.553134  3.194855   \n",
      "22          23  1.777956  0.767272  1.156873  1.629719  2.492699  3.028134   \n",
      "23          24  1.699521  0.728748  1.174258  1.592569  2.301862  3.034483   \n",
      "24          25  6.898882  4.502196  8.489219  5.781793  2.759034  3.525255   \n",
      "25          26  6.231041  4.174325  7.785018  5.427179  2.579319  3.215724   \n",
      "26          27  5.203826  3.168214  5.478121  4.667740  2.489866  3.161299   \n",
      "27          28       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "28          29  4.111519  2.241411  3.838876  3.734154  2.631913  3.352563   \n",
      "29          30  4.245617  2.240047  3.787619  3.634861  2.581994  3.375183   \n",
      "30          31  3.270641  1.675716  2.691482  3.074541  2.686071  3.331467   \n",
      "31          32  3.569938  1.716228  2.825275  3.087027  2.624414  3.251125   \n",
      "32          33  1.779757  0.871218  1.285629  1.717456  2.657046  3.249005   \n",
      "33          34  1.653145  0.748924  1.168300  1.521472  2.369014  3.086714   \n",
      "34          35  1.797816  0.846295  1.293835  1.739028  2.585017  3.462747   \n",
      "35          36  1.669888  0.796665  1.166572  1.680027  2.495357  3.063483   \n",
      "36          37  1.622320  0.710415  1.094824  1.538620  2.370503  3.035615   \n",
      "37          38  1.858093  0.805065  1.238034  1.699404  2.654121  3.422443   \n",
      "38          39  1.603650  0.753189  1.123140  1.529678  2.373317  2.889794   \n",
      "39          40  1.778355  0.801539  1.183935  1.715440  2.453295  3.197499   \n",
      "40          41  1.910079  0.878049  1.279067  1.771966  2.627290  3.211676   \n",
      "41          42  1.820194  0.885695  1.302806  1.759731  2.603043  3.363663   \n",
      "42          43  1.594004  0.744085  1.068087  1.626422  2.491602  3.145747   \n",
      "43          44  1.612012  0.710764  1.032995  1.410764  2.358892  2.833541   \n",
      "44          45  1.751470  0.691403  1.094658  1.527136  2.464514  3.042941   \n",
      "45          46  1.713792  0.783048  1.112815  1.596765  2.384447  3.037830   \n",
      "46          47  1.774833  0.784615  1.093197  1.617875  2.451934  3.142108   \n",
      "\n",
      "          V7        V8        V9  ...      V108      V109      V110      V111  \\\n",
      "0   0.699058  1.224100  1.395388  ...  0.397231  1.510718  0.474696  1.371852   \n",
      "1   0.763314  1.358913  1.395525  ...  0.345163  1.368684  0.425324  1.116855   \n",
      "2   0.716964  1.205908  1.478153  ...  0.339563  1.318642  0.402885  1.147844   \n",
      "3   0.685854  1.147642  1.443252  ...  0.370132  1.300061  0.428895  1.146581   \n",
      "4   0.720883  1.218087  1.601041  ...  0.356426  1.308787  0.397729  1.254237   \n",
      "5   0.660363  1.146805  1.496287  ...  0.371188  1.259393  0.408414  1.208861   \n",
      "6   0.646278  1.122946  1.499165  ...  0.310625  1.201536  0.361277  1.072207   \n",
      "7   0.603491  1.071168  1.493942  ...  0.359214  1.401016  0.443069  1.247199   \n",
      "8   0.680592  1.136441  1.515823  ...  0.348626  1.309689  0.430342  1.149551   \n",
      "9   0.714937  1.100868  1.436365  ...  0.356710  1.209176  0.415572  1.171673   \n",
      "10  0.740999  1.093222  1.607274  ...  0.362056  1.236502  0.420665  1.176091   \n",
      "11  0.653586  1.130284  1.555353  ...  0.355159  1.264943  0.406414  1.170839   \n",
      "12  0.711823  1.102347  1.627617  ...  0.363543  1.270784  0.410271  1.205319   \n",
      "13  0.687262  1.074159  1.540724  ...  0.361355  1.231864  0.419059  1.139674   \n",
      "14  0.741038  1.175138  1.624933  ...  0.369725  1.234215  0.439322  1.210140   \n",
      "15  0.634666  0.995768  1.403584  ...  0.342958  1.332720  0.389986  1.193634   \n",
      "16  0.584709  1.047433  1.761836  ...  0.325761  1.197118  0.404739  1.069126   \n",
      "17  0.668984  1.082870  1.885892  ...  0.362079  1.259410  0.388455  1.130327   \n",
      "18  0.676260  1.031315  1.785682  ...  0.325183  1.180878  0.373319  1.101582   \n",
      "19  0.664747  1.035845  1.705863  ...  0.336535  1.261653  0.412655  1.153258   \n",
      "20  0.689395  1.108135  1.681587  ...  0.349425  1.206267  0.410359  1.186283   \n",
      "21  0.683723  1.102993  1.671188  ...  0.335731  1.235077  0.407470  1.160381   \n",
      "22  0.701745  1.103722  1.587874  ...  0.339429  1.168016  0.400000  1.152006   \n",
      "23  0.638300  1.067398  1.577909  ...  0.329434  1.292056  0.415298  1.183150   \n",
      "24  0.808145  1.376922  2.127171  ...  0.451573  1.464700  0.506804  1.514408   \n",
      "25  0.697577  1.162722  1.955537  ...  0.464212  1.471115  0.524121  1.575471   \n",
      "26  0.718128  1.190472  1.801856  ...  0.373939  1.356823  0.434663  1.275610   \n",
      "27       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "28  0.716149  1.205034  1.742238  ...  0.422399  1.491556  0.501194  1.485625   \n",
      "29  0.707725  1.157232  1.672936  ...  0.376537  1.329246  0.482874  1.332166   \n",
      "30  0.764810  1.230525  1.630283  ...  0.395983  1.347748  0.440585  1.326465   \n",
      "31  0.720345  1.240498  1.748013  ...  0.356267  1.359717  0.472966  1.265303   \n",
      "32  0.721935  1.215366  1.575139  ...  0.464355  1.561787  0.526530  1.163370   \n",
      "33  0.647010  0.992949  1.508104  ...  0.483488  1.590134  0.550482  1.221115   \n",
      "34  0.722392  1.152378  1.565932  ...  0.404246  1.438499  0.483288  1.163608   \n",
      "35  0.673489  1.110858  1.662687  ...  0.415456  1.336821  0.456304  1.138937   \n",
      "36  0.680362  1.063246  1.580425  ...  0.382369  1.310742  0.477614  1.192880   \n",
      "37  0.707547  1.123436  1.594340  ...  0.355939  1.327985  0.454814  1.224439   \n",
      "38  0.603827  1.112332  1.457831  ...  0.335405  1.234274  0.418968  1.149818   \n",
      "39  0.697258  1.131121  1.581818  ...  0.370190  1.230087  0.461061  1.128229   \n",
      "40  0.689979  1.216867  1.675287  ...  0.337243  1.222325  0.402546  1.091017   \n",
      "41  0.760324  1.229599  1.756076  ...  0.361235  1.233408  0.423438  1.131324   \n",
      "42  0.715189  1.081813  1.581723  ...  0.336435  1.210907  0.416696  1.115723   \n",
      "43  0.705148  1.045398  1.456864  ...  0.357834  1.226237  0.440004  1.177846   \n",
      "44  0.674363  1.062367  1.521428  ...  0.350748  1.166569  0.407479  1.148476   \n",
      "45  0.723397  1.036482  1.579746  ...  0.332632  1.157244  0.392876  1.133387   \n",
      "46  0.728057  1.022054  1.604891  ...  0.348046  1.159776  0.446411  1.129053   \n",
      "\n",
      "        V112      V113      V114  V115  V116  V117  \n",
      "0   0.680530  0.483369  0.692406     1     3     1  \n",
      "1   0.550532  0.553482  0.585314     1     3     1  \n",
      "2   0.560453  0.530764  0.589105     1     3     1  \n",
      "3   0.625882  0.576127  0.590501     1     3     1  \n",
      "4   0.577917  0.509750  0.568496     1     3     1  \n",
      "5   0.619082  0.526093  0.579972     1     3     1  \n",
      "6   0.525793  0.533232  0.542919     1     3     1  \n",
      "7   0.644521  0.565367  0.615772     1     3     1  \n",
      "8   0.586166  0.520841  0.620202     1     3     1  \n",
      "9   0.583666  0.557053  0.593375     1     3     1  \n",
      "10  0.572839  0.574792  0.594053     1     3     1  \n",
      "11  0.602326  0.551367  0.588549     1     3     1  \n",
      "12  0.577165  0.559201  0.602109     1     3     1  \n",
      "13  0.568230  0.567039  0.578738     1     3     1  \n",
      "14  0.580157  0.542842  0.593361     1     3     1  \n",
      "15  0.592178  0.584340  0.643805     1     3     1  \n",
      "16  0.565421  0.361912  0.573278     1     3     1  \n",
      "17  0.578293  0.570291  0.606883     1     3     1  \n",
      "18  0.532050  0.528208  0.541202     1     3     1  \n",
      "19  0.591238  0.560601  0.577994     1     3     1  \n",
      "20  0.571688  0.542780  0.570092     1     3     1  \n",
      "21  0.625351  0.568325  0.576284     1     3     1  \n",
      "22  0.577882  0.520492  0.565631     1     3     1  \n",
      "23  0.561739  0.572295  0.586238     1     3     1  \n",
      "24  0.705075  0.663652  0.773593     1     3     1  \n",
      "25  0.754390  0.739373  0.819547     1     3     1  \n",
      "26  0.637990  0.588789  0.631099     1     3     1  \n",
      "27       NaN       NaN       NaN     1     3     1  \n",
      "28  0.718840  0.625628  0.713362     1     3     1  \n",
      "29  0.644481  0.623307  0.670948     1     3     1  \n",
      "30  0.681041  0.607092  0.645743     1     3     1  \n",
      "31  0.622625  0.599448  0.654205     1     3     1  \n",
      "32  0.542859  0.567838  0.593049     1     3     1  \n",
      "33  0.637075  0.572593  0.605369     1     3     1  \n",
      "34  0.580285  0.573340  0.601594     1     3     1  \n",
      "35  0.618455  0.545789  0.567725     1     3     1  \n",
      "36  0.590969  0.576250  0.571627     1     3     1  \n",
      "37  0.614881  0.570384  0.580876     1     3     1  \n",
      "38  0.554679  0.527135  0.532457     1     3     1  \n",
      "39  0.567700  0.553474  0.578919     1     3     1  \n",
      "40  0.586577  0.536999  0.556534     1     3     1  \n",
      "41  0.568527  0.517857  0.604762     1     3     1  \n",
      "42  0.572032  0.552208  0.567559     1     3     1  \n",
      "43  0.610631  0.554020  0.576161     1     3     1  \n",
      "44  0.582611  0.521563  0.564380     1     3     1  \n",
      "45  0.579257  0.541366  0.574111     1     3     1  \n",
      "46  0.573588  0.538758  0.599143     1     3     1  \n",
      "\n",
      "[47 rows x 118 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame() #空的dataframe\n",
    "for i in range(81):\n",
    "    d2=pd.read_csv(files[i])\n",
    "    if d2.isnull().sum().sum() != 0:\n",
    "        print(\"In \",files[i])\n",
    "        for j in range(len(d2.index)) :\n",
    "            if d2.iloc[j].isnull().sum() != 0:\n",
    "                print(\"Nan in row \", j+1 , \" : \" ,  d2.iloc[j].isnull().sum())\n",
    "        print(d2)\n",
    "    df=pd.concat([df,d2],axis=0)  \n",
    "df = df.reset_index()\n",
    "X=df.drop(['Unnamed: 0','V115', 'V116','V117','index'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3807, 114) (3807,)\n",
      "total number of each class:  [3159, 648, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A=np.repeat(0,16)\n",
    "B=np.repeat(1,8)\n",
    "C=np.repeat(0,23)\n",
    "yf=np.hstack((A,B,C))\n",
    "yf=np.tile(yf,81)\n",
    "print('Train:',X.shape,yf.shape)\n",
    "y_num=[(yf==i).sum() for i in range(6)]\n",
    "print(\"total number of each class: \",y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan in row  606  :  114\n",
      "Nan in row  685  :  114\n",
      "(3805, 114) (3805,)\n"
     ]
    }
   ],
   "source": [
    "#%% delete missing data\n",
    "def missing(X):\n",
    "    miss=[]\n",
    "    for i in range(len(X.index)) :\n",
    "        if X.iloc[i].isnull().sum() != 0:\n",
    "            print(\"Nan in row \", i , \" : \" ,  X.iloc[i].isnull().sum())\n",
    "            miss.append(i)\n",
    "    return miss\n",
    "miss=missing(X)\n",
    "X=X.drop(miss)\n",
    "yf=np.delete(yf, miss)\n",
    "print(X.shape,yf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Number of training data: 3424 \n",
      "Number of testing data: 381\n",
      "[2841, 583, 0, 0, 0, 0]\n",
      "[316, 65, 0, 0, 0, 0]\n",
      " \n",
      "===========================================================================\n",
      "Number of training data: 3424 \n",
      "Number of testing data: 381\n",
      "[2841, 583, 0, 0, 0, 0]\n",
      "[316, 65, 0, 0, 0, 0]\n",
      " \n",
      "===========================================================================\n",
      "Number of training data: 3424 \n",
      "Number of testing data: 381\n",
      "[2841, 583, 0, 0, 0, 0]\n",
      "[316, 65, 0, 0, 0, 0]\n",
      " \n",
      "===========================================================================\n",
      "Number of training data: 3424 \n",
      "Number of testing data: 381\n",
      "[2841, 583, 0, 0, 0, 0]\n",
      "[316, 65, 0, 0, 0, 0]\n",
      " \n",
      "===========================================================================\n",
      "Number of training data: 3424 \n",
      "Number of testing data: 381\n",
      "[2841, 583, 0, 0, 0, 0]\n",
      "[316, 65, 0, 0, 0, 0]\n",
      " \n",
      "===========================================================================\n",
      "Number of training data: 3425 \n",
      "Number of testing data: 380\n",
      "[2841, 584, 0, 0, 0, 0]\n",
      "[316, 64, 0, 0, 0, 0]\n",
      " \n",
      "===========================================================================\n",
      "Number of training data: 3425 \n",
      "Number of testing data: 380\n",
      "[2841, 584, 0, 0, 0, 0]\n",
      "[316, 64, 0, 0, 0, 0]\n",
      " \n",
      "===========================================================================\n",
      "Number of training data: 3425 \n",
      "Number of testing data: 380\n",
      "[2842, 583, 0, 0, 0, 0]\n",
      "[315, 65, 0, 0, 0, 0]\n",
      " \n",
      "===========================================================================\n",
      "Number of training data: 3425 \n",
      "Number of testing data: 380\n",
      "[2842, 583, 0, 0, 0, 0]\n",
      "[315, 65, 0, 0, 0, 0]\n",
      " \n",
      "===========================================================================\n",
      "Number of training data: 3425 \n",
      "Number of testing data: 380\n",
      "[2842, 583, 0, 0, 0, 0]\n",
      "[315, 65, 0, 0, 0, 0]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#%% Separate data to to training,validation,testing data\n",
    "### Before applying feature selection method, we need to split the data first.\n",
    "acc_train=[]\n",
    "acc_test=[]\n",
    "sfolder = StratifiedKFold(n_splits=10,random_state=0,shuffle=True)\n",
    "\n",
    "for train_index, test_index in sfolder.split(X,yf):\n",
    "    print(\"===========================================================================\")\n",
    "    #print('Train: %s \\nTest: %s' % (train_index, test_index))\n",
    "    X_train, X_test =X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = np.array(yf)[train_index], np.array(yf)[test_index]\n",
    "    print(\"Number of training data:\",y_train.shape[0],\"\\nNumber of testing data:\",y_test.shape[0])\n",
    "    print([(y_train==i).sum() for i in range(6)])\n",
    "    print([(y_test==i).sum() for i in range(6)])\n",
    "    print(\" \")\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\canni\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    " ### One-hot-encoding\n",
    "num_classes = 6\n",
    "Onehot_train = np_utils.to_categorical(y_train, num_classes)\n",
    "Onehot_test = np_utils.to_categorical(y_test, num_classes)\n",
    "    ### model\n",
    "n=X_train.shape[0]\n",
    "p=X_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(units=int(p*2/3+num_classes),input_dim=p,activation='relu'))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "### Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X_train, Onehot_train, epochs=50, batch_size=25,verbose=0)\n",
    "acc_train.append(round(accuracy_score(y_train,model.predict_classes(X_train)),4))\n",
    "acc_test.append(round(accuracy_score(y_test,model.predict_classes(X_test)),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix of FGFR1 for training data:\n",
      "Accuracy is:  0.9617518248175182\n",
      "predict     0    1\n",
      "label             \n",
      "0        2807   35\n",
      "1          96  487\n",
      "Confusion matrix of FGFR1 for testing data:\n",
      "Accuracy is:  0.9578947368421052\n",
      "predict    0   1\n",
      "label           \n",
      "0        309   6\n",
      "1         10  55\n",
      "ACC_train: [0.9618]  mean: 0.9618\n",
      "ACC_test: [0.9579]  mean: 0.9579\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix of FGFR1 for training data:\")\n",
    "generate_accuracy_and_heatmap(model, X_train, y_train)\n",
    "print(\"Confusion matrix of FGFR1 for testing data:\")\n",
    "generate_accuracy_and_heatmap(model, X_test, y_test)\n",
    "\n",
    "from statistics import mean\n",
    "print(\"ACC_train:\",acc_train,\" mean:\",mean(acc_train))\n",
    "print(\"ACC_test:\",acc_test,\" mean:\",mean(acc_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
